{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e6b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6356fc",
   "metadata": {},
   "source": [
    "# Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenzier function\n",
    "def primitive_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Separates text into tokens by white-space and punctuations.\n",
    "    \"\"\"\n",
    "    normalize_text = text.lower()\n",
    "    white_space_split = normalize_text.split(\" \")\n",
    "\n",
    "    punctuation_split = []\n",
    "    for token in white_space_split:\n",
    "        all_words = \"\"\n",
    "\n",
    "        for char in token:\n",
    "            if char.isalpha() or char.isdigit():\n",
    "                all_words += char\n",
    "\n",
    "            else:\n",
    "                # Split word if non-alphabetical/numerical\n",
    "                all_words += \" \"\n",
    "                all_words += char\n",
    "                all_words += \" \"\n",
    "\n",
    "        # Strip white space at end, split into alphabets/numericals/punctuations\n",
    "        split_words = all_words.strip().split(\" \")\n",
    "        punctuation_split.extend(split_words)\n",
    "\n",
    "    return punctuation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "text = \"NAC has developed a National HIV/AIDS/STI/TB Intervention Strategic Plan (2002-2005) that aims to reduce the HIV prevalence rate among Zambians from 19.3% to 11.7% and improve the health status of people living with HIV/AIDS by 2005.\"\n",
    "primitive_tokenizer_results = primitive_tokenizer(\n",
    "    text\n",
    ")\n",
    "print(\n",
    "    f\"Result of primitive tokenizer is: {primitive_tokenizer_results}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize on input data\n",
    "test_data_file = \"tokens.txt\"\n",
    "num_lines = 10\n",
    "\n",
    "tokenized_result = []\n",
    "with open(test_data_file) as f:\n",
    "    for i in range(num_lines):\n",
    "        line = f.readline()\n",
    "        tokenized_result.extend(\n",
    "            primitive_tokenizer(line)\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Actual test data result: {tokenized_result}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1c77e",
   "metadata": {},
   "source": [
    "# Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens data\n",
    "tokens_file = \"tokens.txt\"\n",
    "tokenized_result = []\n",
    "with open(tokens_file) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        tokenized_result.extend(\n",
    "            primitive_tokenizer(line)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stats\n",
    "num_lines = len(lines)\n",
    "num_token_types = len(list(set(tokenized_result)))\n",
    "num_total_tokens = len(list(tokenized_result))\n",
    "\n",
    "# Obtain token at specific frequency rank\n",
    "token_freq = Counter(tokenized_result)\n",
    "token_sorted = pd.DataFrame.from_records(\n",
    "    list(token_freq.items()),\n",
    "    columns=[\"token\", \"count\"],\n",
    ").sort_values(by=\"count\", ascending=False)\n",
    "token_100s = token_sorted.head(100)\n",
    "token_500 = token_sorted.iloc[499, :]\n",
    "token_1000 = token_sorted.iloc[999, :]\n",
    "token_5000 = token_sorted.iloc[4999, :]\n",
    "token_10000 = token_sorted.iloc[9999, :]\n",
    "\n",
    "# Stats on hapex legomena\n",
    "tokens_single_freq = token_sorted.loc[\n",
    "    token_sorted[\"count\"] == 1\n",
    "]\n",
    "num_single_freq = len(tokens_single_freq)\n",
    "percentage_single_freq = (\n",
    "    num_single_freq * 100 / len(token_sorted)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b627162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(f\"Num lines processed: {num_lines}\")\n",
    "print(f\"Vocabulary size: {num_token_types}\")\n",
    "print(f\"Collection size: {num_total_tokens}\")\n",
    "print(\"\")\n",
    "print(\n",
    "    \"Most freq tokens rank 1-100: please see output csv\"\n",
    ")\n",
    "token_100s.to_csv(\n",
    "    \"Lab1/first100_tokens.csv\"\n",
    ")  # Too long to print\n",
    "print(\"\")\n",
    "print(\"Most freq token at rank 500:\")\n",
    "print(token_500)\n",
    "print(\"\")\n",
    "print(\"Most freq token at rank 1000:\")\n",
    "print(token_1000)\n",
    "print(\"\")\n",
    "print(\"Most freq token at rank 5000:\")\n",
    "print(token_5000)\n",
    "print(\"\")\n",
    "print(\"Most freq token at rank 10000:\")\n",
    "print(token_10000)\n",
    "print(\n",
    "    f\"Num tokens occuring exactly once: {num_single_freq}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of tokens occuringe exactly once: {percentage_single_freq}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38f72e",
   "metadata": {},
   "source": [
    "# Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using output from question (b)\n",
    "zipf_law_data = token_sorted.reset_index()\n",
    "zipf_law_data.plot(\n",
    "    x=\"index\", y=\"count\", kind=\"line\"\n",
    ")\n",
    "\n",
    "# Plot graph\n",
    "plt.title(\"Rank vs frequency of token types\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d18736",
   "metadata": {},
   "source": [
    "# Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a847083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to split sentences\n",
    "def detect_sentence_boundaries(\n",
    "    filepath: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Detects sentences in file, printing number of sentences found and outputs unicode offset of sentence boundaries.\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(\n",
    "        f\"Number of total lines to parse: {len(lines)}\"\n",
    "    )\n",
    "\n",
    "    # Closing punctuation + whitespace + capital letter/number to denote sentence end\n",
    "    end_punctuations = [\n",
    "        \".\",\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \"'\",\n",
    "        '\"',\n",
    "        \";\",\n",
    "        \"-\",\n",
    "    ]\n",
    "    boundary_detections = [\n",
    "        char + \" \" + i\n",
    "        for char in end_punctuations\n",
    "        for i in string.ascii_uppercase\n",
    "    ]\n",
    "    boundary_detections += [\n",
    "        char + \" \" + i\n",
    "        for char in end_punctuations\n",
    "        for i in string.digits\n",
    "    ]\n",
    "    splitting_regex = \"|\".join(\n",
    "        map(re.escape, boundary_detections)\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for i in tqdm(range(len(lines))):\n",
    "        line = lines[i]\n",
    "        sentences = re.split(\n",
    "            splitting_regex, line\n",
    "        )\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        # Calculate offsets excluding last sentence\n",
    "        offset = 0\n",
    "        offset_results = []\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            offset += len(sentence) - 1\n",
    "\n",
    "            # Add offset without whitespace if first sentence\n",
    "            if i != len(sentences) - 1:\n",
    "                offset += 1  # increment to get to end punctuation\n",
    "                offset_results.append(offset)\n",
    "                offset += 3  # increment to get to white space + capital letter\n",
    "\n",
    "            # Offset is last character if last sentence\n",
    "            else:\n",
    "                offset_results.append(\n",
    "                    offset - 1\n",
    "                )  # decrease to minus white space\n",
    "\n",
    "        # Obtain unicode\n",
    "        result = f\"{num_sentences} \"\n",
    "        for offset in offset_results:\n",
    "            result += f\"{offset} \"\n",
    "        result = result.strip()\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Write unicode to txt output\n",
    "    output_filepath = \"Lab1/ID.txt\"\n",
    "    with open(output_filepath, \"w\") as file:\n",
    "        for result in results:\n",
    "            file.write(result + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results on test file\n",
    "test_file = (\n",
    "    \"Lab1/resources/Lab1-If-you-run-on-this.txt\"\n",
    ")\n",
    "test_results = detect_sentence_boundaries(\n",
    "    test_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results on sentences file\n",
    "sentences_file = \"Lab1/resources/sentences.txt\"\n",
    "detect_sentence_boundaries(sentences_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 15 lines to test for program accuracy\n",
    "lines_to_validate = [\n",
    "    random.randint(0, 14980) + 1\n",
    "    for _ in range(15)\n",
    "]\n",
    "print(\n",
    "    f\"Please validate the following lines manually: {lines_to_validate}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b00748",
   "metadata": {},
   "source": [
    "# Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens/sentences data\n",
    "tokens_file = \"Lab1/resources/tokens.txt\"\n",
    "sentences_file = \"Lab1/resources/sentences.txt\"\n",
    "\n",
    "tokens_raw = open(tokens_file).read()\n",
    "sentences_raw = open(sentences_file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word_tokenize to parse files\n",
    "tokens_word = word_tokenize(tokens_raw)\n",
    "sentences_word = word_tokenize(sentences_raw)\n",
    "\n",
    "output_filepath = \"Lab1/tokens_word.txt\"\n",
    "with open(output_filepath, \"w\") as file:\n",
    "    for result in tokens_word:\n",
    "        file.write(result + \"\\n\")\n",
    "output_filepath = \"Lab1/sentences_word.txt\"\n",
    "with open(output_filepath, \"w\") as file:\n",
    "    for result in sentences_word:\n",
    "        file.write(result + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38799a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sent_tokenize to parse files\n",
    "sents_token = sent_tokenize(tokens_raw)\n",
    "sents_sentence = sent_tokenize(sentences_raw)\n",
    "\n",
    "output_filepath = \"Lab1/tokens_sents.txt\"\n",
    "with open(output_filepath, \"w\") as file:\n",
    "    for result in sents_token:\n",
    "        file.write(result + \"\\n\")\n",
    "output_filepath = \"Lab1/sentences_sents.txt\"\n",
    "with open(output_filepath, \"w\") as file:\n",
    "    for result in sents_sentence:\n",
    "        file.write(result + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
