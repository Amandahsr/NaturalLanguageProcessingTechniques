2025-10-23 05:49:55,218 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,222 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'glove'
      (embedding): Embedding(400001, 100)
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)
  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=19, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2025-10-23 05:49:55,223 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,224 Corpus: 14987 train + 3466 dev + 3684 test sentences
2025-10-23 05:49:55,225 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,226 Train:  14987 sentences
2025-10-23 05:49:55,227         (train_with_dev=False, train_with_test=False)
2025-10-23 05:49:55,229 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,230 Training Params:
2025-10-23 05:49:55,231  - learning_rate: "0.1" 
2025-10-23 05:49:55,233  - mini_batch_size: "32"
2025-10-23 05:49:55,233  - max_epochs: "10"
2025-10-23 05:49:55,234  - shuffle: "True"
2025-10-23 05:49:55,235 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,237 Plugins:
2025-10-23 05:49:55,238  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-10-23 05:49:55,238 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,240 Final evaluation on model from best epoch (best-model.pt)
2025-10-23 05:49:55,241  - metric: "('micro avg', 'f1-score')"
2025-10-23 05:49:55,242 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,243 Computation:
2025-10-23 05:49:55,244  - compute on device: cpu
2025-10-23 05:49:55,245  - embedding storage: cpu
2025-10-23 05:49:55,247 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,249 Model training base path: "resources/taggers/test_model1"
2025-10-23 05:49:55,250 ----------------------------------------------------------------------------------------------------
2025-10-23 05:49:55,251 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:02,608 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:02,610 Exiting from training early.
2025-10-23 05:50:02,611 Saving model ...
2025-10-23 05:50:21,193 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,197 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'glove'
      (embedding): Embedding(400001, 100)
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)
  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=19, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2025-10-23 05:50:21,199 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,201 Corpus: 14987 train + 3466 dev + 3684 test sentences
2025-10-23 05:50:21,204 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,206 Train:  14987 sentences
2025-10-23 05:50:21,208         (train_with_dev=False, train_with_test=False)
2025-10-23 05:50:21,209 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,210 Training Params:
2025-10-23 05:50:21,212  - learning_rate: "0.1" 
2025-10-23 05:50:21,214  - mini_batch_size: "32"
2025-10-23 05:50:21,216  - max_epochs: "10"
2025-10-23 05:50:21,217  - shuffle: "True"
2025-10-23 05:50:21,218 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,219 Plugins:
2025-10-23 05:50:21,220  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-10-23 05:50:21,222 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,224 Final evaluation on model from best epoch (best-model.pt)
2025-10-23 05:50:21,225  - metric: "('micro avg', 'f1-score')"
2025-10-23 05:50:21,227 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,228 Computation:
2025-10-23 05:50:21,230  - compute on device: cpu
2025-10-23 05:50:21,231  - embedding storage: cpu
2025-10-23 05:50:21,233 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,234 Model training base path: "resources/taggers/test_model3"
2025-10-23 05:50:21,236 ----------------------------------------------------------------------------------------------------
2025-10-23 05:50:21,237 ----------------------------------------------------------------------------------------------------
2025-10-23 06:03:17,583 epoch 1 - iter 46/469 - loss 0.72426079 - time (sec): 776.34 - samples/sec: 25.81 - lr: 0.100000 - momentum: 0.000000
