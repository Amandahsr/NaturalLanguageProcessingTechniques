{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fb5e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.sparse import hstack, csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad2a29",
   "metadata": {},
   "source": [
    "# Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92d0957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features that are indicative of sentiment\n",
    "train_filepath = \"resources/train.tsv\"\n",
    "\n",
    "categorized_reviews = {\"pos\": [], \"neg\": []}\n",
    "with open(train_filepath, \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in tsv_reader:\n",
    "        star = row[0]\n",
    "        review = row[2]\n",
    "\n",
    "        if star == \"4\":\n",
    "            categorized_reviews[\"pos\"].append(review.lower())\n",
    "        else:\n",
    "            categorized_reviews[\"neg\"].append(review.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbf5aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word   pos   neg\n",
      "0   terrible   0.3   3.6\n",
      "1       good  56.9  43.8\n",
      "2      soggy   0.3   1.6\n",
      "3      bland   1.9   9.2\n",
      "4        bad   7.6  17.3\n",
      "5  expensive   3.3   3.8\n",
      "6    average   3.7   5.3\n",
      "7       slow   2.8   6.4\n",
      "8       like  35.4  43.6\n",
      "9    amazing   5.7   2.1\n"
     ]
    }
   ],
   "source": [
    "# Quantify freq of 10 features\n",
    "features = [\n",
    "    \"terrible\",\n",
    "    \"good\",\n",
    "    \"soggy\",\n",
    "    \"bland\",\n",
    "    \"bad\",\n",
    "    \"expensive\",\n",
    "    \"average\",\n",
    "    \"slow\",\n",
    "    \"like\",\n",
    "    \"amazing\",\n",
    "]\n",
    "\n",
    "feature_counts = []\n",
    "for feature in features:\n",
    "    positive_freq = len(\n",
    "        [\n",
    "            rev\n",
    "            for rev in categorized_reviews[\"pos\"]\n",
    "            if feature in rev\n",
    "        ]\n",
    "    )\n",
    "    negative_freq = len(\n",
    "        [\n",
    "            rev\n",
    "            for rev in categorized_reviews[\"neg\"]\n",
    "            if feature in rev\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    feature_counts.append(\n",
    "        {\n",
    "            \"word\": feature,\n",
    "            \"pos\": positive_freq * 100 / 1000,\n",
    "            \"neg\": negative_freq * 100 / 1000,\n",
    "        }\n",
    "    )\n",
    "\n",
    "feature_counts = pd.DataFrame(feature_counts)\n",
    "\n",
    "print(feature_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12eaad4",
   "metadata": {},
   "source": [
    "# Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd4440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize training/dev/test data by label and description\n",
    "train_filepath = \"resources/train.tsv\"\n",
    "dev_filepath = \"resources/dev.tsv\"\n",
    "test_filepath = \"resources/test.tsv\"\n",
    "\n",
    "train_data = []\n",
    "with open(train_filepath, \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in tsv_reader:\n",
    "        star = row[0]\n",
    "        review = row[2]\n",
    "\n",
    "        train_data.append(\n",
    "            {\"label\": star, \"review\": review.lower()}\n",
    "        )\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "\n",
    "dev_data = []\n",
    "with open(dev_filepath, \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in tsv_reader:\n",
    "        star = row[0]\n",
    "        doc_id = row[1]\n",
    "        review = row[2]\n",
    "\n",
    "        dev_data.append(\n",
    "            {\n",
    "                \"label\": star,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"review\": review.lower(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "dev_data = pd.DataFrame(dev_data)\n",
    "\n",
    "test_data = []\n",
    "with open(test_filepath, \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in tsv_reader:\n",
    "        star = row[0]\n",
    "        doc_id = row[1]\n",
    "        review = row[2]\n",
    "\n",
    "        test_data.append(\n",
    "            {\n",
    "                \"label\": star,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"review\": review.lower(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f29dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to bag of words representation\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_data[\"review\"])\n",
    "\n",
    "# Convert to tf-idf representation\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(\n",
    "    train_counts\n",
    ")\n",
    "train_tf = tf_transformer.transform(train_counts)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb_model = MultinomialNB().fit(train_tf, train_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab48870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of training set: 11468\n",
      "Feature representation of first doc in dev set...\n",
      "again - 0.07293249574894728\n",
      "and - 0.4375949744936837\n",
      "average - 0.07293249574894728\n",
      "both - 0.07293249574894728\n",
      "business - 0.07293249574894728\n",
      "chef - 0.07293249574894728\n",
      "chicken - 0.07293249574894728\n",
      "china - 0.14586499149789456\n",
      "deserves - 0.07293249574894728\n",
      "does - 0.07293249574894728\n",
      "enjoyed - 0.07293249574894728\n",
      "fan - 0.07293249574894728\n",
      "first - 0.07293249574894728\n",
      "food - 0.07293249574894728\n",
      "forward - 0.07293249574894728\n",
      "friendly - 0.07293249574894728\n",
      "general - 0.07293249574894728\n",
      "good - 0.14586499149789456\n",
      "grand - 0.07293249574894728\n",
      "great - 0.07293249574894728\n",
      "have - 0.14586499149789456\n",
      "if - 0.07293249574894728\n",
      "in - 0.07293249574894728\n",
      "is - 0.14586499149789456\n",
      "it - 0.14586499149789456\n",
      "items - 0.07293249574894728\n",
      "just - 0.07293249574894728\n",
      "lady - 0.07293249574894728\n",
      "look - 0.07293249574894728\n",
      "management - 0.07293249574894728\n",
      "menu - 0.21879748724684184\n",
      "new - 0.07293249574894728\n",
      "not - 0.21879748724684184\n",
      "of - 0.07293249574894728\n",
      "on - 0.14586499149789456\n",
      "once - 0.07293249574894728\n",
      "online - 0.07293249574894728\n",
      "opened - 0.07293249574894728\n",
      "original - 0.14586499149789456\n",
      "other - 0.07293249574894728\n",
      "pepper - 0.07293249574894728\n",
      "phone - 0.07293249574894728\n",
      "place - 0.07293249574894728\n",
      "portion - 0.07293249574894728\n",
      "price - 0.07293249574894728\n",
      "really - 0.07293249574894728\n",
      "recent - 0.07293249574894728\n",
      "restaurant - 0.07293249574894728\n",
      "return - 0.07293249574894728\n",
      "she - 0.07293249574894728\n",
      "site - 0.07293249574894728\n",
      "size - 0.07293249574894728\n",
      "so - 0.07293249574894728\n",
      "steak - 0.07293249574894728\n",
      "tasty - 0.07293249574894728\n",
      "the - 0.4375949744936837\n",
      "their - 0.14586499149789456\n",
      "this - 0.07293249574894728\n",
      "to - 0.14586499149789456\n",
      "told - 0.07293249574894728\n",
      "trying - 0.07293249574894728\n",
      "tso - 0.07293249574894728\n",
      "under - 0.07293249574894728\n",
      "wanted - 0.07293249574894728\n",
      "web - 0.07293249574894728\n",
      "were - 0.14586499149789456\n",
      "what - 0.07293249574894728\n",
      "when - 0.07293249574894728\n",
      "years - 0.07293249574894728\n",
      "yet - 0.07293249574894728\n",
      "you - 0.07293249574894728\n",
      "your - 0.07293249574894728\n"
     ]
    }
   ],
   "source": [
    "# Total num features\n",
    "print(\n",
    "    f\"Vocab size of training set: {len(count_vect.vocabulary_)}\"\n",
    ")\n",
    "\n",
    "# Feature representation of first doc in dev set\n",
    "first_doc_features = count_vect.transform(\n",
    "    [dev_data[\"review\"][0]]\n",
    ")\n",
    "first_doc_tf = tf_transformer.transform(first_doc_features)\n",
    "\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "nonzero_idx = first_doc_tf.nonzero()[1]\n",
    "feature_values = first_doc_tf.data\n",
    "\n",
    "print(\"Feature representation of first doc in dev set...\")\n",
    "for idx, value in zip(nonzero_idx, feature_values):\n",
    "    print(f\"{feature_names[idx]} - {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593e810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for first 10 docs in dev set...\n",
      "DocID: ZSJnW6faaNFQoqq4ALqYg, Predicted label: 4\n",
      "DocID: Rcbv11hm5AYEwZyqYwAvg, Predicted label: 2\n",
      "DocID: rkRTjhu5szaBggeFVcVJlA, Predicted label: 4\n",
      "DocID: dhmeDsQGUS1FXMLs49SWjQ, Predicted label: 4\n",
      "DocID: z9zfIMYmRRCE4ggfOIieEw, Predicted label: 4\n",
      "DocID: Xtb3pGSh39bqcozkBECw, Predicted label: 2\n",
      "DocID: DOUflAGzxLsXG6xOmR1w, Predicted label: 2\n",
      "DocID: 0RxCEWURe08CTcZt95F4AQ, Predicted label: 2\n",
      "DocID: MzUg5twEcCyd0X6lBMP2Lg, Predicted label: 2\n",
      "DocID: uNlw2D5CYKk0wjNxLtYw, Predicted label: 4\n"
     ]
    }
   ],
   "source": [
    "# Predict on first 10 docs of dev data\n",
    "dev_features = count_vect.transform(dev_data[\"review\"][:10])\n",
    "dev_predictions = nb_model.predict(dev_features)\n",
    "\n",
    "print(\"Predictions for first 10 docs in dev set...\")\n",
    "for doc_id, pred in zip(\n",
    "    dev_data[\"doc_id\"][:10], dev_predictions\n",
    "):\n",
    "    print(f\"DocID: {doc_id}, Predicted label: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3432e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on full dev data\n",
    "dev_features = count_vect.transform(dev_data[\"review\"])\n",
    "dev_predicted = nb_model.predict(dev_features)\n",
    "dev_results = pd.DataFrame(\n",
    "    {\"doc_id\": dev_data[\"doc_id\"], \"predicted\": dev_predicted}\n",
    ")\n",
    "\n",
    "dev_results.to_csv(\"nb_prediction_dev.csv\", index=False)\n",
    "\n",
    "# Generate predictions on full test data\n",
    "test_features = count_vect.transform(test_data[\"review\"])\n",
    "test_predicted = nb_model.predict(test_features)\n",
    "test_results = pd.DataFrame(\n",
    "    {\"doc_id\": test_data[\"doc_id\"], \"predicted\": test_predicted}\n",
    ")\n",
    "\n",
    "test_results.to_csv(\"nb_prediction_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1fc71",
   "metadata": {},
   "source": [
    "# Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2014d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set results (Positive class) - Precision: 0.8661137440758294, Recall: 0.731, F1: 0.7928416485900217\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(pred, actual, pos_label=\"4\"):\n",
    "    \"\"\"\n",
    "    Returns evaluation metrics on predictions.\n",
    "    \"\"\"\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    true_neg = 0\n",
    "\n",
    "    # Extract prediction counts\n",
    "    for p, a in zip(pred, actual):\n",
    "        if p == pos_label and a == pos_label:\n",
    "            true_pos += 1\n",
    "        elif p == pos_label and a != pos_label:\n",
    "            false_pos += 1\n",
    "        elif p != pos_label and a == pos_label:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluate for positive class on dev set\n",
    "dev_features = count_vect.transform(dev_data[\"review\"])\n",
    "dev_predictions = nb_model.predict(dev_features)\n",
    "precision, recall, f1 = evaluate_predictions(\n",
    "    dev_predictions, dev_data[\"label\"]\n",
    ")\n",
    "print(\n",
    "    f\"Dev set results (Positive class) - Precision: {precision}, Recall: {recall}, F1: {f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ac58373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incorrect predictions on dev set\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for pred, actual, review, doc_id in zip(\n",
    "    dev_predictions,\n",
    "    dev_data[\"label\"],\n",
    "    dev_data[\"review\"],\n",
    "    dev_data[\"doc_id\"],\n",
    "):\n",
    "    if (pred == \"4\") & (actual == \"2\"):\n",
    "        false_positives.append(\n",
    "            {\"doc_id\": doc_id, \"review\": review}\n",
    "        )\n",
    "    if (pred == \"2\") & (actual == \"4\"):\n",
    "        false_negatives.append(\n",
    "            {\"doc_id\": doc_id, \"review\": review}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1686663",
   "metadata": {},
   "source": [
    "# Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c593aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVM classifier\n",
    "svm_model = LinearSVC().fit(train_tf, train_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76f7ec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for first 10 docs in dev set...\n",
      "DocID: ZSJnW6faaNFQoqq4ALqYg, Predicted label: 4\n",
      "DocID: Rcbv11hm5AYEwZyqYwAvg, Predicted label: 4\n",
      "DocID: rkRTjhu5szaBggeFVcVJlA, Predicted label: 2\n",
      "DocID: dhmeDsQGUS1FXMLs49SWjQ, Predicted label: 4\n",
      "DocID: z9zfIMYmRRCE4ggfOIieEw, Predicted label: 4\n",
      "DocID: Xtb3pGSh39bqcozkBECw, Predicted label: 2\n",
      "DocID: DOUflAGzxLsXG6xOmR1w, Predicted label: 2\n",
      "DocID: 0RxCEWURe08CTcZt95F4AQ, Predicted label: 2\n",
      "DocID: MzUg5twEcCyd0X6lBMP2Lg, Predicted label: 2\n",
      "DocID: uNlw2D5CYKk0wjNxLtYw, Predicted label: 2\n"
     ]
    }
   ],
   "source": [
    "# Predict on first 10 docs of dev data\n",
    "dev_features = count_vect.transform(dev_data[\"review\"][:10])\n",
    "dev_predictions = svm_model.predict(dev_features)\n",
    "\n",
    "print(\"Predictions for first 10 docs in dev set...\")\n",
    "for doc_id, pred in zip(\n",
    "    dev_data[\"doc_id\"][:10], dev_predictions\n",
    "):\n",
    "    print(f\"DocID: {doc_id}, Predicted label: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "166d7f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set results (Positive class) - Precision: 0.8096135721017907, Recall: 0.859, F1: 0.8335759340126153\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(pred, actual, pos_label=\"4\"):\n",
    "    \"\"\"\n",
    "    Returns evaluation metrics on predictions.\n",
    "    \"\"\"\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    true_neg = 0\n",
    "\n",
    "    # Extract prediction counts\n",
    "    for p, a in zip(pred, actual):\n",
    "        if p == pos_label and a == pos_label:\n",
    "            true_pos += 1\n",
    "        elif p == pos_label and a != pos_label:\n",
    "            false_pos += 1\n",
    "        elif p != pos_label and a == pos_label:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluate for positive class on dev set\n",
    "dev_features = count_vect.transform(dev_data[\"review\"])\n",
    "dev_predictions = svm_model.predict(dev_features)\n",
    "precision, recall, f1 = evaluate_predictions(\n",
    "    dev_predictions, dev_data[\"label\"]\n",
    ")\n",
    "print(\n",
    "    f\"Dev set results (Positive class) - Precision: {precision}, Recall: {recall}, F1: {f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74a30695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 'PmWYwnFJKAM9JQx2vAUgeg',\n",
       "  'review': \"leaving the club...looking for late night eats...and in-n-out's closed!  this chicago kid was ready to commit ritualistic suicide on the spot.  well, it turned out to be a blessing in disguise, as we came here, and i finally got to try carne asada fries.  this leads me to believe that tacos mexico used to be known as tacos france (seriously though...what a stupid name).  whatever...even though they weren't on the menu, they sure did serve the fries.  and while my californian friends remarked that it's made a lot better near them, i had no complaints.  they were cheap, plentiful, and damn tasty.\"},\n",
       " {'doc_id': 'UZ1BbY6z4dbakukZhDXpLQ',\n",
       "  'review': 'i have not been to this place in like 10 years.  got invited to go to lunch today and it was exactly the same as it was the last time i was there.  buffet was pretty good and the menu has not changed either.  not much to look at but for a 6.00 lunch what do you expect.'},\n",
       " {'doc_id': 'Lxd9H2RZZWwUoGngKV9NQ',\n",
       "  'review': \"i used to rock this joint on lunch at moon valley high school across the street.  this is the best -bertos type place in the area, not fili b's, not ramiro's, not rolando's.  humberto's.  best carne asada in the area.\"},\n",
       " {'doc_id': 'RR9VVx2qeUxUk3IRSoEJA',\n",
       "  'review': \"ok, good news first.  the pan roast is still the best you will find anywhere even though the price is now up to $17.99.  the bad news is that you can only get the pan roast now at the bar.  you can no longer order pan roast if you go inside the restaurant and sit down.  the wait is also excruciating.  i went there on a wednesday night.  not exactly a time where the casino is packed.  there was only 1 waitress and 1 cook for 18 seats.  i counted because i was standing in line for 40 minutes!  i just stood there and watch customer after customer get up and leave after they finished their meal but there was no one to clear out their plates and tell the next person to sit down.  it was hard for me to get mad at the waitress since she had to take all the orders, bring out all the drinks, take care of all the checks, serve soup & salad, and clear the plates.   so after 10 seats opened up, the waitress finally got around to seating the 6 people waiting in line.  then, of course, there was another 20 minute wait for the food.  total time waiting 1 hr 30 minutes.... on a wednesday night!  if you want a good pan roast go here.  just don't expect to get it very quickly.\"},\n",
       " {'doc_id': '02H1RTIh9tO5igxcBMI9w',\n",
       "  'review': \"didn't have big expectations but was pleasantly surprised. we were looking for a quick lunch before a spring training game.  our server/ bartender recommended the shrimp tacos and they did not disappoint. flicka himself brought us our food, introduced himself and chatted with us a bit. looks like it would be a fun night time spot.\"}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze incorrect predictions on dev set\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for pred, actual, review, doc_id in zip(\n",
    "    dev_predictions,\n",
    "    dev_data[\"label\"],\n",
    "    dev_data[\"review\"],\n",
    "    dev_data[\"doc_id\"],\n",
    "):\n",
    "    if (pred == \"4\") & (actual == \"2\"):\n",
    "        false_positives.append(\n",
    "            {\"doc_id\": doc_id, \"review\": review}\n",
    "        )\n",
    "    if (pred == \"2\") & (actual == \"4\"):\n",
    "        false_negatives.append(\n",
    "            {\"doc_id\": doc_id, \"review\": review}\n",
    "        )\n",
    "\n",
    "false_negatives[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e4fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on full dev data\n",
    "dev_features = count_vect.transform(dev_data[\"review\"])\n",
    "dev_predicted = svm_model.predict(dev_features)\n",
    "dev_results = pd.DataFrame(\n",
    "    {\"doc_id\": dev_data[\"doc_id\"], \"predicted\": dev_predicted}\n",
    ")\n",
    "\n",
    "dev_results.to_csv(\"svm_prediction_dev.csv\", index=False)\n",
    "\n",
    "# Generate predictions on full test data\n",
    "test_features = count_vect.transform(test_data[\"review\"])\n",
    "test_predicted = svm_model.predict(test_features)\n",
    "test_results = pd.DataFrame(\n",
    "    {\"doc_id\": test_data[\"doc_id\"], \"predicted\": test_predicted}\n",
    ")\n",
    "\n",
    "test_results.to_csv(\"svm_prediction_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9ff92",
   "metadata": {},
   "source": [
    "# Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13213b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize SentiWordNet data\n",
    "swn_filepath = \"resources/SentiWordNet_3.0.0_20130122.txt\"\n",
    "\n",
    "swn_data = []\n",
    "with open(swn_filepath, \"r\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip().startswith(\"#\"):\n",
    "            line_data = line.strip().split(\"\\t\")\n",
    "\n",
    "            # Split synset terms that have multiple words\n",
    "            terms = [\n",
    "                re.sub(r\"#\\d+$\", \"\", term)\n",
    "                for term in line_data[4].split()\n",
    "            ]\n",
    "            for term in terms:\n",
    "                swn_data.append(\n",
    "                    {\n",
    "                        \"PosScore\": float(line_data[2]),\n",
    "                        \"NegScore\": float(line_data[3]),\n",
    "                        \"SynsetTerms\": term,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "swn_data = pd.DataFrame(swn_data)\n",
    "\n",
    "# Remove words that have zero pos/neg scores\n",
    "swn_data = swn_data[\n",
    "    (swn_data[\"PosScore\"] > 0) | (swn_data[\"NegScore\"] > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b055413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [41:57<00:00,  1.26s/it] \n"
     ]
    }
   ],
   "source": [
    "def compute_sentiment_score(\n",
    "    review: str, swn_data: pd.DataFrame\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns sentiment score (pos-neg scores) for a review.\n",
    "    \"\"\"\n",
    "    swn_terms = swn_data[\"SynsetTerms\"].unique()\n",
    "\n",
    "    # Extract words from review\n",
    "    words = re.findall(r\"\\b\\w+\\b\", review)\n",
    "    total_score = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in swn_terms:\n",
    "            pos_score = swn_data.loc[\n",
    "                swn_data[\"SynsetTerms\"] == word\n",
    "            ][\"PosScore\"].values[0]\n",
    "            neg_score = swn_data.loc[\n",
    "                swn_data[\"SynsetTerms\"] == word\n",
    "            ][\"NegScore\"].values[0]\n",
    "            review_score = pos_score - neg_score\n",
    "            total_score += review_score\n",
    "\n",
    "    return total_score\n",
    "\n",
    "\n",
    "# Generate sentiment scores for all reviews in training data\n",
    "sentiment_scores = [\n",
    "    compute_sentiment_score(r, swn_data)\n",
    "    for r in tqdm(train_data[\"review\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fdffe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_bigrams_tokenizer(review: str):\n",
    "    \"\"\"\n",
    "    Bigram tokenizer that handles negation.\n",
    "    \"\"\"\n",
    "    negation_words = [\"n't\", \"not\", \"no\", \"never\"]\n",
    "\n",
    "    # Strip review to words\n",
    "    words = re.findall(r\"\\w+(?:'\\w+)*|[^\\w\\s]\", review)\n",
    "    prepend_negation = False\n",
    "\n",
    "    bigrams = []\n",
    "    for word in words:\n",
    "        # keep already negated words\n",
    "        if any(nw in word for nw in negation_words):\n",
    "            bigrams.append(word)\n",
    "            prepend_negation = True\n",
    "\n",
    "        # Stop negation when punctuation reached\n",
    "        elif word in [\".\", \"!\", \"?\", \",\", \";\", \":\"]:\n",
    "            prepend_negation = False\n",
    "            bigrams.append(word)\n",
    "\n",
    "        # Switch on negation\n",
    "        elif prepend_negation:\n",
    "            # Tag word with NOT_\n",
    "            bigrams.append(\"NOT_\" + word)\n",
    "\n",
    "        # Regular word\n",
    "        else:\n",
    "            bigrams.append(word)\n",
    "\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06c9c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_sparse = csr_matrix(\n",
    "    np.array(sentiment_scores)\n",
    ").reshape(-1, 1)\n",
    "\n",
    "# Generate bag of words representation\n",
    "bag_of_words = count_vect.transform(train_data[\"review\"])\n",
    "\n",
    "# Generate bigrams with negation handling\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=negation_bigrams_tokenizer, ngram_range=(1, 2)\n",
    ")\n",
    "negation_bigrams = vectorizer.fit_transform(train_data[\"review\"])\n",
    "\n",
    "# Combine all features\n",
    "training_features = hstack(\n",
    "    [bag_of_words, sentiment_sparse, negation_bigrams]\n",
    ")\n",
    "\n",
    "# Train SVM with all features\n",
    "svm_model_v2 = LinearSVC().fit(\n",
    "    training_features, train_data[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed3f79e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [39:57<00:00,  1.20s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set results (Positive class) - Precision: 0.851, Recall: 0.851, F1: 0.851\n"
     ]
    }
   ],
   "source": [
    "# Obtain features for dev set\n",
    "dev_bag_of_words = count_vect.transform(dev_data[\"review\"])\n",
    "dev_sentiment_scores = [\n",
    "    compute_sentiment_score(r, swn_data)\n",
    "    for r in tqdm(dev_data[\"review\"])\n",
    "]\n",
    "dev_sentiment_sparse = csr_matrix(\n",
    "    np.array(dev_sentiment_scores).reshape(-1, 1)\n",
    ")\n",
    "dev_negation_bigrams = vectorizer.transform(dev_data[\"review\"])\n",
    "dev_features = hstack(\n",
    "    [\n",
    "        dev_bag_of_words,\n",
    "        dev_sentiment_sparse,\n",
    "        dev_negation_bigrams,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predict on dev set\n",
    "dev_predictions = svm_model_v2.predict(dev_features)\n",
    "\n",
    "precision, recall, f1 = evaluate_predictions(\n",
    "    dev_predictions, dev_data[\"label\"]\n",
    ")\n",
    "print(\n",
    "    f\"Dev set results (Positive class) - Precision: {precision}, Recall: {recall}, F1: {f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b9602d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set results (Positive class) - Precision: 0.851, Recall: 0.851, F1: 0.851\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(pred, actual, pos_label=\"4\"):\n",
    "    \"\"\"\n",
    "    Returns evaluation metrics on predictions.\n",
    "    \"\"\"\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    true_neg = 0\n",
    "\n",
    "    # Extract prediction counts\n",
    "    for p, a in zip(pred, actual):\n",
    "        if p == pos_label and a == pos_label:\n",
    "            true_pos += 1\n",
    "        elif p == pos_label and a != pos_label:\n",
    "            false_pos += 1\n",
    "        elif p != pos_label and a == pos_label:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "precision, recall, f1 = evaluate_predictions(\n",
    "    dev_predictions, dev_data[\"label\"]\n",
    ")\n",
    "print(\n",
    "    f\"Dev set results (Positive class) - Precision: {precision}, Recall: {recall}, F1: {f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40f50807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [39:15<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on full test data\n",
    "test_bag_of_words = count_vect.transform(test_data[\"review\"])\n",
    "test_sentiment_scores = [\n",
    "    compute_sentiment_score(r, swn_data)\n",
    "    for r in tqdm(test_data[\"review\"])\n",
    "]\n",
    "test_sentiment_sparse = csr_matrix(\n",
    "    np.array(test_sentiment_scores).reshape(-1, 1)\n",
    ")\n",
    "test_negation_bigrams = vectorizer.transform(test_data[\"review\"])\n",
    "test_features = hstack(\n",
    "    [\n",
    "        test_bag_of_words,\n",
    "        test_sentiment_sparse,\n",
    "        test_negation_bigrams,\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_predicted = svm_model_v2.predict(test_features)\n",
    "test_results = pd.DataFrame(\n",
    "    {\"doc_id\": test_data[\"doc_id\"], \"predicted\": test_predicted}\n",
    ")\n",
    "\n",
    "test_results.to_csv(\"svm_v2_prediction_test.csv\", index=False)\n",
    "\n",
    "with open(\"svm_v2_prediction_test.txt\", \"w\") as file:\n",
    "    for id, pred in zip(test_data[\"doc_id\"], test_predicted):\n",
    "        file.write(f\"{id}\\t{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
