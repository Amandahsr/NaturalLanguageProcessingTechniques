{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2837394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from resources.charlm import (\n",
    "    train_char_lm,\n",
    "    print_probs,\n",
    "    generate_text,\n",
    "    perplexity,\n",
    "    smoothed_perplexity,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d80b6",
   "metadata": {},
   "source": [
    "# Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a7d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train order 4 model on subtitles.txt\n",
    "subtitles_file = \"resources/subtitles.txt\"\n",
    "model = train_char_lm(subtitles_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35efaa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuations of atio:\n",
      "[('n', 0.9940436161014506),\n",
      " (' ', 0.00220962628494572),\n",
      " ('.', 0.0013930252665962147),\n",
      " (',', 0.0009607070804111826),\n",
      " ('?', 0.0003362474781439139),\n",
      " (\"'\", 0.00024017677010279565),\n",
      " ('u', 0.00019214141608223654),\n",
      " ('\"', 0.0001441060620616774),\n",
      " ('s', 0.0001441060620616774),\n",
      " ('-', 9.607070804111827e-05),\n",
      " ('!', 4.8035354020559135e-05),\n",
      " (':', 4.8035354020559135e-05),\n",
      " ('m', 4.8035354020559135e-05),\n",
      " ('p', 4.8035354020559135e-05),\n",
      " ('r', 4.8035354020559135e-05)]\n",
      "Continuations of nivi:\n",
      "[('n', 0.8), ('e', 0.1), ('s', 0.1)]\n",
      "Continuations of supe:\n",
      "[('r', 0.9992144540455616), ('s', 0.0007855459544383347)]\n"
     ]
    }
   ],
   "source": [
    "# continuations of words\n",
    "print(\"Continuations of atio:\")\n",
    "print_probs(model, \"atio\")\n",
    "\n",
    "print(\"Continuations of nivi:\")\n",
    "print_probs(model, \"nivi\")\n",
    "\n",
    "print(\"Continuations of supe:\")\n",
    "print_probs(model, \"supe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae345b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Uh...\n",
      "May away every drug-dee?\n",
      "A few mortgage.\n",
      "Just not a pickets own some even.\n",
      "Iteration 2: Anythink of the disarmed?\n",
      "Wentz?\n",
      "Depechecking, \"Titus rice to be a should chairc\n",
      "Iteration 3: To be a man.\n",
      "Yeah, they're staff.\n",
      "He door, the help.\n",
      "I did you.\n",
      "l'm gone most on\n",
      "Iteration 4: I've good.\n",
      "THE BEG YOU JUST BOLO and spell me were injury anyone with the pie Zi\n",
      "Iteration 5: We cas impale Roy. I loves shows.\n",
      "Two den night, you can't separtner.\n",
      "The fucked\n",
      "Iteration 6: Tell day be our breat\n",
      "Don't know what you have go with a bomb was back, after fr\n",
      "Iteration 7: CAPTURE, BUT YOU DON'T LIKE THE NEW MORNISHING LIKE A GOOD LOVE YOU BELIEVE ME.\n",
      "\n",
      "Iteration 8: 'Cause now.\n",
      "Well, I got evening to the kids of their own a party?\n",
      "Henry she in t\n",
      "Iteration 9: Maybe someteorism, but I can I went out on it he's the code at the long the Colo\n",
      "Iteration 10: Come pen.\n",
      "Racherry.\n",
      "I maybe ther's gave a pornograph?\n",
      "Actually trying you show t\n"
     ]
    }
   ],
   "source": [
    "# Produce random strings\n",
    "num_strings = 10\n",
    "num_char = 80\n",
    "\n",
    "for i in range(num_strings):\n",
    "    random_str = generate_text(model, 4, num_char)\n",
    "    print(f\"Iteration {i+1}: {random_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d01b0",
   "metadata": {},
   "source": [
    "# Question (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a33d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of 'The boy loves his mother': 3.909190367374623\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity of test string\n",
    "test_string = \"The boy loves his mother\"\n",
    "test_perplexity = perplexity(\n",
    "    test_string, model, 4\n",
    ")\n",
    "print(\n",
    "    f\"Perplexity of '{test_string}': {test_perplexity}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12c511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of 'The student loves homework': 4.606972940490916\n",
      "Perplexity of 'The yob loves homework': inf\n",
      "Perplexity of 'It is raining in London': 3.7112360009044525\n",
      "Perplexity of 'asdfjkl; qwerty': inf\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity of specified strings\n",
    "strings = [\n",
    "    \"The student loves homework\",\n",
    "    \"The yob loves homework\",\n",
    "    \"It is raining in London\",\n",
    "    \"asdfjkl; qwerty\",\n",
    "]\n",
    "\n",
    "for s in strings:\n",
    "    perplexity_result = perplexity(s, model, 4)\n",
    "    print(\n",
    "        f\"Perplexity of '{s}': {perplexity_result}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587c6b8",
   "metadata": {},
   "source": [
    "# Question (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8acbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed perplexity of 'The student loves homework': 4.606972940490916\n",
      "Smoothed perplexity of 'The yob loves homework': 1e-07\n",
      "Smoothed perplexity of 'It is raining in London': 3.7112360009044525\n",
      "Smoothed perplexity of 'asdfjkl; qwerty': 1e-07\n"
     ]
    }
   ],
   "source": [
    "# Calculate smoothed perplexity of specified strings\n",
    "strings = [\n",
    "    \"The student loves homework\",\n",
    "    \"The yob loves homework\",\n",
    "    \"It is raining in London\",\n",
    "    \"asdfjkl; qwerty\",\n",
    "]\n",
    "\n",
    "for s in strings:\n",
    "    smoothed_perplexity_result = (\n",
    "        smoothed_perplexity(s, model, 4)\n",
    "    )\n",
    "    print(\n",
    "        f\"Smoothed perplexity of '{s}': {smoothed_perplexity_result}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa49b4",
   "metadata": {},
   "source": [
    "# Question (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7caa407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:18<00:00,  6.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train unigram/bigram/4-gram models on training set of 6 languages\n",
    "language_names = [\n",
    "    \"da\",\n",
    "    \"de\",\n",
    "    \"en\",\n",
    "    \"fr\",\n",
    "    \"it\",\n",
    "    \"nl\",\n",
    "]\n",
    "orders = [0, 1, 3]\n",
    "\n",
    "# Store all trained models according to order and language\n",
    "trained_language_models = {}\n",
    "for order in tqdm(orders):\n",
    "    trained_models = {}\n",
    "    for lang in language_names:\n",
    "        training_file = (\n",
    "            f\"resources/{lang}.train.txt\"\n",
    "        )\n",
    "        trained_models[lang] = train_char_lm(\n",
    "            training_file, order=order\n",
    "        )\n",
    "\n",
    "    trained_language_models[order] = (\n",
    "        trained_models\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(\n",
    "    text: str,\n",
    "    trained_language_models: Dict,\n",
    "    order: int,\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Returns language code of model with lowest smoothed perplexity \n",
    "        on input text, and perplexity scores of all models.\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    lowest_perplexity = float(\"inf\")\n",
    "    perplexities = {}\n",
    "\n",
    "    # Track model with lowest perplexity\n",
    "    for lang in trained_language_models.keys():\n",
    "        model = trained_language_models[lang]\n",
    "        perplexity = smoothed_perplexity(\n",
    "            text, model, order=order\n",
    "        )\n",
    "        perplexities[lang] = perplexity\n",
    "\n",
    "        if perplexity < lowest_perplexity:\n",
    "            lowest_perplexity = perplexity\n",
    "            best_model = lang\n",
    "\n",
    "    # Return language code of model with lowest perplexity\n",
    "    return best_model, perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "619bae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for order 0 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 58.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores of all models on first test line: {'da': 28.990266275714845, 'de': 29.177491185275965, 'en': 1e-07, 'fr': 21.23036140889091, 'it': 23.153186591077052, 'nl': 26.319382689734336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:07, 167.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracies for the 6 languages are:\n",
      "  language  total_lines  correct_lines  accuracy\n",
      "0       da          200             18       9.0\n",
      "1       de          200             43      21.5\n",
      "2       en          200            171      85.5\n",
      "3       fr          200             18       9.0\n",
      "4       it          200             95      47.5\n",
      "5       nl          200            159      79.5\n",
      "Making predictions for order 1 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 291.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores of all models on first test line: {'da': 1e-07, 'de': 1e-07, 'en': 1e-07, 'fr': 10.56989170441308, 'it': 1e-07, 'nl': 22.47677726796925}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:02, 427.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracies for the 6 languages are:\n",
      "  language  total_lines  correct_lines  accuracy\n",
      "0       da          200             29      14.5\n",
      "1       de          200             27      13.5\n",
      "2       en          200             59      29.5\n",
      "3       fr          200             11       5.5\n",
      "4       it          200             64      32.0\n",
      "5       nl          200             35      17.5\n",
      "Making predictions for order 3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [00:00, 1363.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores of all models on first test line: {'da': 1e-07, 'de': 1e-07, 'en': 1e-07, 'fr': 1e-07, 'it': 1e-07, 'nl': 1e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:00, 2241.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracies for the 6 languages are:\n",
      "  language  total_lines  correct_lines  accuracy\n",
      "0       da          200            136      68.0\n",
      "1       de          200              0       0.0\n",
      "2       en          200              0       0.0\n",
      "3       fr          200              0       0.0\n",
      "4       it          200              0       0.0\n",
      "5       nl          200              0       0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse test file\n",
    "test_file = \"resources/test.txt\"\n",
    "orders = [0, 1, 3]\n",
    "\n",
    "# Make predictions for models of different orders\n",
    "for order in orders:\n",
    "    print(\n",
    "        f\"Making predictions for order {order} model...\"\n",
    "    )\n",
    "\n",
    "    testing_results = []\n",
    "    with open(test_file) as f:\n",
    "        # For each line, get lowest perplexity model\n",
    "        for idx, line in tqdm(enumerate(f)):\n",
    "            true_lang, text = line.strip().split(\n",
    "                \"\\t\"\n",
    "            )\n",
    "            predicted_lang, perplexities = (\n",
    "                predict_language(\n",
    "                    text,\n",
    "                    trained_language_models[\n",
    "                        order\n",
    "                    ],\n",
    "                    order=order,\n",
    "                )\n",
    "            )\n",
    "            testing_results.append(\n",
    "                {\n",
    "                    \"correct_lang\": true_lang,\n",
    "                    \"predicted_lang\": predicted_lang,\n",
    "                    \"result\": int(\n",
    "                        true_lang\n",
    "                        == predicted_lang\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Obtain perplexity scores of all models on first line\n",
    "            if idx == 0:\n",
    "                print(\n",
    "                    f\"Perplexity scores of all models on first test line: {perplexities}\"\n",
    "                )\n",
    "\n",
    "    testing_results = pd.DataFrame(\n",
    "        testing_results\n",
    "    )\n",
    "\n",
    "    # For each language, compute accuracy\n",
    "    accuracies = []\n",
    "    for (\n",
    "        group,\n",
    "        group_table,\n",
    "    ) in testing_results.groupby(\n",
    "        [\"correct_lang\"]\n",
    "    ):\n",
    "        language = group[0]\n",
    "        total_lines = len(group_table)\n",
    "        correct_lines = len(\n",
    "            group_table.loc[\n",
    "                group_table[\"result\"] == 1\n",
    "            ]\n",
    "        )\n",
    "        accuracy = (\n",
    "            correct_lines * 100 / total_lines\n",
    "        )\n",
    "        accuracies.append(\n",
    "            {\n",
    "                \"language\": language,\n",
    "                \"total_lines\": total_lines,\n",
    "                \"correct_lines\": correct_lines,\n",
    "                \"accuracy\": accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    accuracies = pd.DataFrame(accuracies)\n",
    "\n",
    "    print(\n",
    "        \"The accuracies for the 6 languages are:\"\n",
    "    )\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f4063",
   "metadata": {},
   "source": [
    "# Question (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdce049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train set into male/female strings\n",
    "train_file = \"resources/tennis.train.txt\"\n",
    "\n",
    "female_lines = []\n",
    "male_lines = []\n",
    "with open(train_file) as f:\n",
    "    for line in f:\n",
    "\n",
    "        # Case normalize text\n",
    "        speaker, text = line.strip().split(\"\\t\")\n",
    "        if speaker == \"F\":\n",
    "            female_lines.append(text.lower())\n",
    "        else:\n",
    "            male_lines.append(text.lower())\n",
    "\n",
    "# Write new train sets\n",
    "with open(\"female.tennis.train.txt\", \"w\") as f:\n",
    "    for line in female_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(\"male.tennis.train.txt\", \"w\") as f:\n",
    "    for line in male_lines:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3040c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:18<00:00,  6.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train unigram/bigram/5-gram female and male models\n",
    "speakers = [\"female\", \"male\"]\n",
    "orders = [0, 1, 4]\n",
    "\n",
    "# Store all trained models according to order and speaker\n",
    "trained_speaker_models = {}\n",
    "for order in tqdm(orders):\n",
    "    trained_models = {}\n",
    "    for speaker in speakers:\n",
    "        training_file = (\n",
    "            f\"{speaker}.tennis.train.txt\"\n",
    "        )\n",
    "        trained_models[speaker] = train_char_lm(\n",
    "            training_file, order=order\n",
    "        )\n",
    "\n",
    "    trained_speaker_models[order] = trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_speaker(\n",
    "    text: str,\n",
    "    trained_speaker_models: Dict,\n",
    "    order: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Returns speaker with lowest smoothed perplexity on \n",
    "        input text.\n",
    "    \"\"\"\n",
    "    best_speaker = None\n",
    "    lowest_perplexity = float(\"inf\")\n",
    "\n",
    "    # Track speaker with lowest perplexity\n",
    "    for speaker in trained_speaker_models.keys():\n",
    "        model = trained_speaker_models[speaker]\n",
    "        perplexity = smoothed_perplexity(\n",
    "            text, model, order=order\n",
    "        )\n",
    "\n",
    "        if perplexity < lowest_perplexity:\n",
    "            lowest_perplexity = perplexity\n",
    "            best_speaker = speaker\n",
    "\n",
    "    # Map to speaker code\n",
    "    if best_speaker == \"female\":\n",
    "        best_speaker = \"F\"\n",
    "    else:\n",
    "        best_speaker = \"M\"\n",
    "\n",
    "    # Return speaker with lowest perplexity\n",
    "    return best_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f8c83d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for order 0 model...\n",
      "The accuracies for the speaker models are:\n",
      "  speaker  total_lines  correct_lines   accuracy\n",
      "0       F         3696           1876  50.757576\n",
      "1       M         4518           2541  56.241700\n",
      "Making predictions for order 1 model...\n",
      "The accuracies for the speaker models are:\n",
      "  speaker  total_lines  correct_lines   accuracy\n",
      "0       F         3696           2271  61.444805\n",
      "1       M         4518           2616  57.901726\n",
      "Making predictions for order 4 model...\n",
      "The accuracies for the speaker models are:\n",
      "  speaker  total_lines  correct_lines   accuracy\n",
      "0       F         3696           2645  71.563853\n",
      "1       M         4518           1466  32.447986\n"
     ]
    }
   ],
   "source": [
    "# Parse test file\n",
    "test_file = \"resources/tennis.test.txt\"\n",
    "orders = [0, 1, 4]\n",
    "\n",
    "# Make predictions for models of different orders\n",
    "for order in orders:\n",
    "    print(\n",
    "        f\"Making predictions for order {order} model...\"\n",
    "    )\n",
    "\n",
    "    testing_results = []\n",
    "    with open(test_file) as f:\n",
    "        # For each line, get lowest perplexity model\n",
    "        for line in f:\n",
    "            true_speaker, text = (\n",
    "                line.strip().split(\"\\t\")\n",
    "            )\n",
    "            predicted_speaker = predict_speaker(\n",
    "                text.lower(),\n",
    "                trained_speaker_models[order],\n",
    "                order=order,\n",
    "            )\n",
    "            testing_results.append(\n",
    "                {\n",
    "                    \"correct_speaker\": true_speaker,\n",
    "                    \"predicted_speaker\": predicted_speaker,\n",
    "                    \"result\": int(\n",
    "                        true_speaker\n",
    "                        == predicted_speaker\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    testing_results = pd.DataFrame(\n",
    "        testing_results\n",
    "    )\n",
    "\n",
    "    # For each speaker, compute accuracy\n",
    "    accuracies = []\n",
    "    for (\n",
    "        group,\n",
    "        group_table,\n",
    "    ) in testing_results.groupby(\n",
    "        [\"correct_speaker\"]\n",
    "    ):\n",
    "        speaker = group[0]\n",
    "        total_lines = len(group_table)\n",
    "        correct_lines = len(\n",
    "            group_table.loc[\n",
    "                group_table[\"result\"] == 1\n",
    "            ]\n",
    "        )\n",
    "        accuracy = (\n",
    "            correct_lines * 100 / total_lines\n",
    "        )\n",
    "        accuracies.append(\n",
    "            {\n",
    "                \"speaker\": speaker,\n",
    "                \"total_lines\": total_lines,\n",
    "                \"correct_lines\": correct_lines,\n",
    "                \"accuracy\": accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    accuracies = pd.DataFrame(accuracies)\n",
    "\n",
    "    print(\n",
    "        \"The accuracies for the speaker models are:\"\n",
    "    )\n",
    "    print(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
